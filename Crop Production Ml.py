# -*- coding: utf-8 -*-
"""Crop Production ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMhBLGzNxN5djTG0gIRhfAfDO5_HU3Ab

# **Preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from scipy import stats

from google.colab import files
import io
import pandas as pd


uploaded = files.upload()
for file_name in uploaded.keys():
    data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=',')

raw_data = pd.read_csv('/content/Raw Data (3).csv', delimiter=',')

raw_data.head()

print(raw_data.info())

"""Scaling"""

# List of columns to standardize (all columns except 'Crop')
numerical_columns = ['Nitrogen', 'Phosphorus', 'Potassium', 'Temperature', 'Humidity', 'pH', 'Rainfall']

# Initialize the StandardScaler
scaler = StandardScaler()

# Standardize the numerical columns and keep the 'Crop' column intact
raw_data[numerical_columns] = scaler.fit_transform(raw_data[numerical_columns])

# The 'Crop' column remains unchanged
print(raw_data.head())

cleaned_data = raw_data.copy()

from google.colab import files

cleaned_data.to_csv('data.csv', index=False)
files.download('data.csv')

"""# **EDA**"""

data = cleaned_data.drop(columns=['Crop'])

# Compute the correlation matrix
correlation_matrix = data.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# Add title
plt.title('Correlation Matrix of Features', size=15)

# Show the plot
plt.show()

"""# **Clustering**"""

!pip install kneed

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy import stats
import numpy as np
from kneed import KneeLocator

# Set the style of seaborn plots
sns.set(style="whitegrid")

# Plot box plots for each feature in the dataset
# We will exclude the 'Cluster' column if already added, and any non-numeric columns
features = data.select_dtypes(include=[np.number]).columns.tolist()  # Select numeric columns

plt.figure(figsize=(16, 12))  # Adjust size for better readability
for i, feature in enumerate(features, 1):
    plt.subplot(3, 3, i)  # Adjust subplot grid as per the number of features
    sns.boxplot(data=cleaned_data, x=feature)
    plt.title(f'Box plot of {feature}')

plt.tight_layout()
plt.show()

"""Evaluate the clusters using elbow method"""

# Calculate WCSS for a range of cluster numbers
wcss = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

# Plot the WCSS values
plt.plot(range(1, 10), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using the KneeLocator
knee = KneeLocator(range(1, 10), wcss, curve='convex', direction='decreasing')
optimal_clusters = knee.elbow

print(f"Optimal number of clusters: {optimal_clusters}")

# Step 1: Remove outliers based on Z-Score threshold (e.g., |z| > 3 is considered an outlier)
z_scores = np.abs(stats.zscore(data))  # Get absolute z-scores
filtered_entries = (z_scores < 3).all(axis=1)  # Filter rows where all z-scores are within 3
data_filtered = data[filtered_entries].copy()  # Create a filtered DataFrame

# Step 2: Apply K-Means Clustering with k-means++ initialization to find 4 clusters
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)
kmeans.fit(data_filtered)

# Step 3: Map the cluster labels to 'Very Low Yield', 'Low Yield', 'Medium Yield', 'High Yield'
yield_labels = ['Low', 'Medium', 'High']
data_filtered['Yield'] = [yield_labels[label] for label in kmeans.labels_]  # Add 'Yield' column with labels

# Step 4: Visualize Clusters using PCA (reducing to 2D for visualization purposes)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(data_filtered.drop(columns=['Yield']))  # Exclude 'Yield' for PCA

plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', edgecolor='k', s=50)
plt.title('Clustering Results (K-Means)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.legend(handles=scatter.legend_elements()[0], labels=yield_labels, title="Yield Levels")
plt.show()

"""Cluster the Independent Features"""

columns_to_cluster = ['Nitrogen', 'Phosphorus', 'Potassium', 'Temperature', 'Humidity', 'pH', 'Rainfall']

# Loop through each column to apply K-Means independently
for column in columns_to_cluster:
    # Initialize KMeans with 3 clusters
    kmeans = KMeans(n_clusters=3, random_state=42)

    # Reshape the column into a 2D array for fitting KMeans
    kmeans_labels = kmeans.fit_predict(cleaned_data[[column]])

    # Map the numeric labels to 'Low', 'Medium', and 'High'
    cluster_mapping = {0: 'Low', 1: 'Medium', 2: 'High'}
    cleaned_data[column + ' Level'] = pd.Series(kmeans_labels).map(cluster_mapping)

# Step 5: Merge the new 'Yield' column back into the original dataset if needed
cleaned_data['Yield'] = np.nan
cleaned_data.loc[filtered_entries, 'Yield'] = data_filtered['Yield']  # Only assign to non-outliers

cleaned_data.head()

unique, counts = np.unique(kmeans.labels_, return_counts=True)
cluster_sizes = dict(zip(unique, counts))
print("Cluster sizes:", cluster_sizes)

data_filtered['Cluster'] = kmeans.labels_

# Calculate the means of each feature for each admission status and cluster
cluster_means = data_filtered.groupby(['Cluster', 'Yield']).mean().reset_index()

# Display the means
print(cluster_means)

final_data = cleaned_data.dropna()

final_data.info()

from google.colab import files

final_data.to_csv('final_data.csv', index=False)
files.download('final_data.csv')

"""# **Supervised Learning Using Clustered 'Admission' Results**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

from google.colab import files
import io
import pandas as pd


uploaded = files.upload()
for file_name in uploaded.keys():
    data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=',')

data = pd.read_csv('/content/Final_Data.csv', delimiter=',')

data.head()

"""Data Augmentation for ML"""

from imblearn.over_sampling import SMOTE

# Assuming your dataset is called `data`
# 1. One-Hot Encode the 'Yield' Column
data['Yield'] = data['Yield'].astype('category')

# Apply Label Encoding to the 'Yield' column (to prepare for ML)
label_encoder = LabelEncoder()
data['Yield'] = label_encoder.fit_transform(data['Yield'])

# 2. Balance the dataset using SMOTE (for oversampling the minority class)
# Assuming 'data' has features and 'Yield' as the target variable.
X = data.drop(columns=['Yield', 'Crop'])  # Features
y = data['Yield']  # Target labels

# Perform SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine the resampled features and labels back into a DataFrame
augmented_data = pd.DataFrame(X_resampled, columns=X.columns)
augmented_data['Yield'] = y_resampled

# Display the resampled dataset
print(augmented_data['Yield'].value_counts())

# Preview the augmented dataset
print(augmented_data.head())

data = augmented_data.copy()

data.head()

# Define features (X) and target (y)

X = data.drop(columns=['Yield'])  # Features (everything except the target column)+
y = data['Yield']  # Target column

# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Build the deep learning model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer
    layers.Dropout(0.5),  # Dropout to avoid overfitting
    layers.Dense(32, activation='relu'),  # Hidden layer
    layers.Dropout(0.5),  # Another Dropout layer
    layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=10,
                    batch_size=32,
                    callbacks=[early_stopping])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# After evaluating the model on the test set, generate predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32")  # Convert probabilities to binary predictions

# Generate a classification report (including precision, recall, and F1-score)
report = classification_report(y_test, y_pred, target_names=['Low', 'Medium', 'High'])
print(report)

"""The was no effect of data augmentation on the prediction of high yield."""